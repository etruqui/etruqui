---
  title: "happiness_project"
output: html_document
---
  
  
  
```{r happiness, echo=FALSE}
#LOADING DATA AND DOWNLOADING PACKAGES

data <- read.csv('https://raw.githubusercontent.com/arunagossai/happiness_project_R/master/happiness_data.csv', header = TRUE)
head(data)
summary(data)
dim(data)
#install.packages('fastDummies')
#install.packages('qdapTools')
#install.packages('glmnet')
#install.packages('tseries')
library(tseries)
library(fastDummies)
library(qdapTools)
library(glmnet)
```



```{r happiness, echo=FALSE}
#VARIABLE CREATION AND PREPROCESSING

#Creating a key for the regions, will use later
regionkey = levels(data$region)
regionvalues = c(1:10)
key = data.frame(regionkey,regionvalues)

#Changing the region from categorical to numeric
data$region <- as.numeric(data$region)

#Change getting rid of countries with no observations, and countries with no democracy value
df1  = subset(data, country != "Kosovo"  & country !="Taiwan" & country!="Sudan" & democracy!= 'NA')
paste( dim(data)[1] - dim(df1)[1], "observations lost")

#Taking the mean of each column by country. Changes dataset from pooled cross-sectional to cross-sectional 
df2 <- aggregate.data.frame(df1[-2], by = list(df1$country), mean)
paste( dim(df1)[1] - dim(df2)[1], "observations lost")

#adding a column for the region name
rname = lookup(df2$region,key$regionvalues,key$regionkey)
df = data.frame(df2,rname)

#Creating dummy variables from the region name
df_dum <- dummy_cols(df, select_columns = "rname")

#testing for multicollinearity excluding regions and year from matrix
cor(df[6:15])
#serious issues with multicollinearity, dropping the problem variables
df$men_edu <- NULL
df$sanitation <- NULL
df$elder_child <- NULL
df$child_mortality <- NULL
df_dum$men_edu <- NULL
df_dum$sanitation <- NULL
df_dum$elder_child <- NULL
df_dum$child_mortality <- NULL

#dropping variables that are not needed
df$year <- NULL
df$ï..id <- NULL
df$region <- NULL
df_dum$year <- NULL
df_dum$ï..id <- NULL
df_dum$region <- NULL
df_dum$rname <- NULL  
df_dum$rname_West_EU <- NULL #getting rid of one dummy variable to prevent multicollinearity

#creating binary 'very happy' variables for classification models
df_dum$veryhappy <- ifelse(df_dum$happiness >= 6.5,1,0)
df$veryhappy <- ifelse(df$happiness >= 6.5,1,0)


#rename countries column
colnames(df)
names(df)[names(df)=="Group.1"]<-"countries"
names(df)[names(df)=="rname"]<-"region"
head(df)
  
###NOTE####
#we now have two datasets. One has a column for region as a dummy variable, one does not. you can view them with the code below. Please remember that we have decided NOT to get rid of the  countries with outliers. Your two options are: replacing outliers with NAs or keeping them as is
```



```{r}
#MODEL 1: LINEAR MODEL WITH ALL VARIABLES
set.seed(56)

n = nrow(df_dum)
Index = sample(1:n, size = round(0.7*n), replace=FALSE)
train = df_dum[Index,]
test = df_dum[-Index,]  




head(df_dum)
M2 = lm(happiness ~ ., train[2:17])
pred_base = predict(M2, test[2:17])
#RMSE out prediction
RMSE_BASE = sqrt(sum((pred_base-test$happiness)^2)/length(pred_base))
RMSE_BASE
```



```{r}
##Practice splitting into train/test sets
trainSize<-round(nrow(df)*0.7)
testSize<-nrow(df)-trainSize

set.seed(123)
training_indices <- sample(seq_len(nrow(df)), size=trainSize)
 trainSet <- df[training_indices, ]
 testSet <- df[-training_indices, ]

```

```{r}
#MODEL 2: CLASSIFICATION MODEL
#LOGISTIC REGRESSION MODEL: CLASSIFICATION#
###########################################

#some exploratory analysis
head(df)
summary(df)
dim(df)
View(df)

#builds a logistic regression model treating the region as a numerical variable
M1.1<- glm(veryhappy ~ .-countries, data = df, family = "binomial")
summary(M1.1)
View(exp(predict(M1.1, df)))
df$predicM1.1<-exp(predict(M1.1, df, digits=3))

#df1$admit <- factor(df1$admit) #transforms admit into a factor (categorical) variable
df$veryhappy <- factor(df$veryhappy) #transforms rank into a factor (categorical) variable
head(df)
class(df$veryhappy)

#builds the model where veryhappy is now categorical (a factor variable)
M1.2<-glm(veryhappy ~ democracy + gini + refugee + women_edu + pop_den + labour, data = df, family = "binomial")
summary(M1.2)
df$predicM1.2<-exp(predict(M1.2, df, digits=3))

#construct confidence intervals using one of two methods:
confint(M1.2)  #using profiled log-likelihood
#confint.defaults(M1.2) #using standard errors ---cannot find function

#look at confidence interval next to point estimates
point_conf_table<-cbind(M1.2$coefficients, confint(M1.2))
point_conf_table

#converting values to odds-ratio interpretation
exp(point_conf_table)


library(caret)  #calls the caret library to use createDataPartition()
set.seed(123) #locks seed for random partitioning

#now we will split the data into training and test sets
#creates a vector of rows to randomly sample p=70% from the raw data for traning
inTrain <- createDataPartition(y=df$veryhappy, p=.70, list = FALSE) 

#stores these rows in the training set
Training<-df[inTrain,]  

#stores all rows not in the training set in the test/validation set
Testing<-df[-inTrain,]  

##LOGISTIC REGRESSION## (need rank to be factor)
M_LOG<-glm(veryhappy ~ democracy + gini + refugee + women_edu + pop_den + labour, data = Training, family = "binomial")
summary(M_LOG)
exp(cbind(M_LOG$coefficients, confint(M_LOG)))
#TRAINING DATA
confusionMatrix(table(predict(M_LOG, Training, type="response") >= 0.5,
                      Training$veryhappy == 1))
#TESTING DATA
confusionMatrix(table(predict(M_LOG, Testing, type="response") >= 0.5,
                      Testing$veryhappy == 1))

#GETTING CONFIDENCE INTERVALS
cbind(M_LOG$coefficients,confint(M_LOG))







```

```{r}
#CLASSIFICATION MODEL
###

library(caret)  #calls the caret library to use createDataPartition()
set.seed(456) #locks seed for random partitioning

#now we will split the data into training and test sets
#creates a vector of rows to randomly sample p=70% from the raw data for traning
inTrain <- createDataPartition(y=df$veryhappy, p=.70, list = FALSE) 

#stores these rows in the training set
Training<-df[inTrain,]  
dim(Training)
#stores all rows not in the training set in the test/validation set
Testing<-df[-inTrain,]  
dim(Testing)

#partition training set into training and validation (test) sets
inTrain <- createDataPartition(y=Training$veryhappy, p=.70, list = FALSE)
TrainTrain<-Training[inTrain,]
TrainValid<-Training[-inTrain,]

#check the dimensions of the training data partitions
dim(TrainTrain)
dim(TrainValid)

#all variables in model
print(colnames(Training))

#check the dimensions of the training partition at each transformation
dim(df)
dim(Training)
dim(TrainTrain)
dim(TrainValid)
dim(Testing)

#removes near-zero variance variables
NZV<-nearZeroVar(TrainTrain, saveMetrics=TRUE)
TrainTrain2<-TrainTrain[,!NZV$nzv]
cleanTrain<-TrainTrain2
#the remaining variables after the transformations and processing
print(colnames(cleanTrain))

#Changing the region from categorical to numeric
cleanTrain$region <- as.numeric(cleanTrain$region)

#Next, we apply the same transformations to the TrainValid and Testing sets
cleanedcol<-colnames(cleanTrain)
cleanValid<-TrainValid[,cleanedcol]
cleancolValid<-colnames(cleanTrain[,-(length(cleanTrain))])
cleanTesting<-Testing[,cleanedcol]
dim(cleanTesting)
colnames(cleanTesting)

##checking dimensions of the cleaned training partitions and test cases
dim(cleanTrain)
dim(cleanValid)
dim(cleanTesting)


##Random Forests
#We compute two random forest models using both the caret and rattle package implementations and compare results on the cleanedTest set as well as on the 20 test set cases provided for the assignment to check for agreement in the predictions across package implementations - there are slight differences between caret and rattle implementations of the RF algorithm.  In the case of the caret implementation we include preprocessing (mean-variance normalization) and 3-fold cross validation.
#caret implementation
RF1 <- train(veryhappy ~ .-countries, method="rf", trControl=trainControl(method = "cv", number = 3), preProcess=c("center", "scale"), data=cleanTrain)
print(RF1, digits=3)

##confusion matrix for predicting from the test data from the training set partition
RF1pred <- predict(RF1, newdata=cleanValid)
print(confusionMatrix(RF1pred, cleanValid$veryhappy), digits=3) 
class(RF1pred)
class(cleanValid)

#Next, we repeat using the rattle implementation of the RF algorithm:
##rattle implementation
RF2 <- randomForest(veryhappy ~.-countries, data=cleanTrain)
print(RF2, digits=3)

#Next, we compare to the performance on the testing partition of the training data:
##confusion matrix for predicting from the test data from the training set partition
RF2pred <- predict(RF2, cleanValid, type = "class")
confusionMatrix(RF2pred, cleanValid$veryhappy) 

```
